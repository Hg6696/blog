import{_ as a,o as t,c as l,ag as r}from"./chunks/framework.DEqXEGcv.js";const p=JSON.parse('{"title":"大语言模型(LLM)的原理解密","description":"","frontmatter":{},"headers":[],"relativePath":"ai/models/llm-explained.md","filePath":"ai/models/llm-explained.md","lastUpdated":null}'),o={name:"ai/models/llm-explained.md"};function n(i,e,s,d,m,_){return t(),l("div",null,[...e[0]||(e[0]=[r('<h1 id="大语言模型-llm-的原理解密" tabindex="-1">大语言模型(LLM)的原理解密 <a class="header-anchor" href="#大语言模型-llm-的原理解密" aria-label="Permalink to &quot;大语言模型(LLM)的原理解密&quot;">​</a></h1><p>大语言模型（LLM, Large Language Model）是当前 AI 热潮的核心引擎。本文带你拆解其背后的黑盒。</p><h2 id="transformer-架构" tabindex="-1">Transformer 架构 <a class="header-anchor" href="#transformer-架构" aria-label="Permalink to &quot;Transformer 架构&quot;">​</a></h2><p>自 2017 年 Google 提出《Attention Is All You Need》以来，Transformer 已成为 NLP 的绝对霸主。</p><ul><li><strong>自注意力机制 (Self-Attention)</strong>：让模型在处理每一个词时，都能“看”到上下文中的其他词，从而理解语境。</li><li><strong>多头注意力 (Multi-Head Attention)</strong>：允许模型同时关注句子的不同方面（如语法、情感、指代等）。</li></ul><h2 id="训练三步曲" tabindex="-1">训练三步曲 <a class="header-anchor" href="#训练三步曲" aria-label="Permalink to &quot;训练三步曲&quot;">​</a></h2><ol><li><strong>预训练 (Pre-training)</strong>：使用海量互联网数据，让模型学会“预测下一个词”（Next Token Prediction）。</li><li><strong>监督微调 (SFT)</strong>：使用高质量的问答对进行训练，让模型学会遵循指令。</li><li><strong>人类反馈强化学习 (RLHF)</strong>：通过奖励模型（Reward Model）让 AI 的回答更符合人类价值观，避免有害内容输出。</li></ol>',7)])])}const f=a(o,[["render",n]]);export{p as __pageData,f as default};
