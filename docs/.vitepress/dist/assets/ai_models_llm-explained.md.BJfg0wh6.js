import{_ as t,o as r,c as a,ag as o}from"./chunks/framework.DEqXEGcv.js";const _=JSON.parse('{"title":"大语言模型(LLM)的原理解密","description":"深入解析Transformer架构与大语言模型的训练三步曲。","frontmatter":{"title":"大语言模型(LLM)的原理解密","description":"深入解析Transformer架构与大语言模型的训练三步曲。","cover":"https://picsum.photos/seed/llm/600/400"},"headers":[],"relativePath":"ai/models/llm-explained.md","filePath":"ai/models/llm-explained.md","lastUpdated":1771829663000}'),l={name:"ai/models/llm-explained.md"};function n(i,e,s,d,m,p){return r(),a("div",null,[...e[0]||(e[0]=[o('<p>大语言模型（LLM, Large Language Model）是当前 AI 热潮的核心引擎。本文带你拆解其背后的黑盒。</p><h2 id="transformer-架构" tabindex="-1">Transformer 架构 <a class="header-anchor" href="#transformer-架构" aria-label="Permalink to &quot;Transformer 架构&quot;">​</a></h2><p>自 2017 年 Google 提出《Attention Is All You Need》以来，Transformer 已成为 NLP 的绝对霸主。</p><ul><li><strong>自注意力机制 (Self-Attention)</strong>：让模型在处理每一个词时，都能“看”到上下文中的其他词，从而理解语境。</li><li><strong>多头注意力 (Multi-Head Attention)</strong>：允许模型同时关注句子的不同方面（如语法、情感、指代等）。</li></ul><h2 id="训练三步曲" tabindex="-1">训练三步曲 <a class="header-anchor" href="#训练三步曲" aria-label="Permalink to &quot;训练三步曲&quot;">​</a></h2><ol><li><strong>预训练 (Pre-training)</strong>：使用海量互联网数据，让模型学会“预测下一个词”（Next Token Prediction）。</li><li><strong>监督微调 (SFT)</strong>：使用高质量的问答对进行训练，让模型学会遵循指令。</li><li><strong>人类反馈强化学习 (RLHF)</strong>：通过奖励模型（Reward Model）让 AI 的回答更符合人类价值观，避免有害内容输出。</li></ol>',6)])])}const f=t(l,[["render",n]]);export{_ as __pageData,f as default};
